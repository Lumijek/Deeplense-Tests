{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a139ae3",
   "metadata": {},
   "source": [
    "# Specific Test IV. Diffusion Models \n",
    "\n",
    "\n",
    "Task: Develop a generative model to simulate realistic strong gravitational lensing images. Train a diffusion model (DDPM) to generate lensing images. You are encouraged to explore various architectures and implementations within the diffusion model framework. Please implement your approach in PyTorch or Keras and discuss your strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0db04d9",
   "metadata": {},
   "source": [
    "## Step 1: Imports\n",
    "We start by importing everything we will need to work with and visualize the data. I am using PyTorch to create my final solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb012350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "torchvision.disable_beta_transforms_warning() # this is to disable warnings with torchvision v2 transforms\n",
    "\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from math import pi\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from einops import rearrange, reduce\n",
    "from einops.layers.torch import Rearrange\n",
    "from ema_pytorch import EMA\n",
    "from torch import einsum\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcc167b",
   "metadata": {},
   "source": [
    "## Creating the dataset\n",
    "\n",
    "This dataset consists of 10,000 images for training the diffusion model which is a bit small but still works. For the transforms, I resize the image from 155 -> 160 (nearest multiple of 32), apply random horizontal flipping, and ensure the dataset follows a standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0062845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrongLensingDataset(Dataset):\n",
    "    def __init__(self, imgs, train=True, transform=None):\n",
    "        self.imgs = imgs\n",
    "        self.len = sum(1 for _, _, files in os.walk(self.imgs) for f in files)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx += 1\n",
    "        path = os.path.join(self.imgs, f\"sample{idx}.npy\")\n",
    "        img = torch.from_numpy(np.load(path))\n",
    "        img = img.to(torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "def get_dataloader(file_path, image_size=160, train=True, batch_size=16, num_workers=2):\n",
    "    transform = v2.Compose(\n",
    "        [\n",
    "            v2.Resize((image_size, image_size), antialias=True), # closest multiple of 32 to 155\n",
    "            v2.RandomHorizontalFlip(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Normalize(mean=[0.0615], std=[0.1164]), # mean = 0, variance = 1\n",
    "        ]\n",
    "    )\n",
    "    transform2 = v2.Compose(\n",
    "        [\n",
    "            v2.Resize((image_size, image_size), antialias=True), # closest multiple of 32 to 155\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Normalize(mean=[0.0615], std=[0.1164]), # mean = 0, variance = 1\n",
    "        ]\n",
    "    )\n",
    "    if train:\n",
    "        dataset = StrongLensingDataset(file_path, train=train, transform=transform)\n",
    "    else:\n",
    "        dataset = StrongLensingDataset(file_path, train=train, transform=transform2)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size, shuffle=True, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc1934",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "For the model, I closely followed openAI's guided diffusion Unet using Phil Wang's implementation for reference(specifically for the linear and normal attention). The model uses Groupnorm and SiLU, alongside a unet that passes 2 residual skip connections for every residual block. The model also takes time embeddings which are inserted into the model at every single residual block using the Adaptive Group Normalization technqiue.\n",
    "\n",
    "The specific improvements from the paper *Diffusion models beat GANs on image synthesis* include AdaGN implemented using a scale and shift, attention layers at every single layer, and BigGan residual blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d4fcaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    def __init__(self, channels, out_channels=None, end=False):\n",
    "        super().__init__()\n",
    "        self.end = end\n",
    "        if not end:\n",
    "            self.upsample = nn.Upsample(scale_factor=2)\n",
    "        if out_channels != None:\n",
    "            self.conv = nn.Conv2d(channels, out_channels, kernel_size=3, padding=1)\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.end:\n",
    "            x = self.upsample(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, channels, out_channels=None, end=False):\n",
    "        super().__init__()\n",
    "        self.end = end\n",
    "        if not end:\n",
    "            self.downsample = nn.AvgPool2d(kernel_size=2)\n",
    "        if out_channels != None:\n",
    "            self.conv = nn.Conv2d(channels, out_channels, kernel_size=3, padding=1)\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.end:\n",
    "            x = self.downsample(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# use same as BigGan according to paper\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_size, groups=32, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_size, out_channels * 2\n",
    "            ),  # times 2 to split later for the AdaGN\n",
    "        )\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.norm1 = nn.GroupNorm(groups, out_channels)\n",
    "        self.act1 = nn.SiLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(groups, out_channels)\n",
    "        self.act2 = nn.SiLU()\n",
    "\n",
    "        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t = self.time_mlp(t)[..., None, None]\n",
    "        scale, shift = t.chunk(2, dim=1)\n",
    "\n",
    "        # First pass\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = out * (scale + 1) + shift  # adagn\n",
    "        out = self.act1(out)\n",
    "        \n",
    "        # Second pass\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.act2(out)\n",
    "        #out = self.dropout(out)\n",
    "\n",
    "        return out + self.shortcut(x)\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x\n",
    "\n",
    "# huggingface blog\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(x, dim=1) * self.g * (x.shape[-1] ** 0.5)\n",
    "\n",
    "#huggingface blog\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = RMSNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)\n",
    "\n",
    "# hugging face blog -> phil wang\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1), RMSNorm(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "\n",
    "        q = q.softmax(dim=-2)\n",
    "        k = k.softmax(dim=-1)\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
    "\n",
    "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
    "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "# huggingface blog -> phil wang\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
    "        attn = sim.softmax(dim=-1)\n",
    "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
    "\n",
    "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        input_channels=3,\n",
    "        output_channels=3,\n",
    "        mid_blocks=1,\n",
    "        channel_mults=[1, 2, 4, 8],\n",
    "        T=1000,\n",
    "        groups=32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.channel_mults = channel_mults\n",
    "        self.channel_mults.insert(0, 1)\n",
    "        self.in_out_channels = []\n",
    "\n",
    "        for i in range(len(channel_mults) - 1):\n",
    "            self.in_out_channels.append(\n",
    "                (int(channels * channel_mults[i]), int(channels * channel_mults[i + 1]))\n",
    "            )\n",
    "\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.middles = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "\n",
    "        self.initial_conv = nn.Conv2d(\n",
    "            input_channels, channels, kernel_size=3, padding=1\n",
    "        )\n",
    "        time_emb_size = channels * 4  # author papers did this\n",
    "\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Embedding(T, channels),\n",
    "            nn.Linear(channels, time_emb_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_size, time_emb_size),\n",
    "        )\n",
    "        current_channel = channels\n",
    "        for i, (in_channel, out_channel) in enumerate(self.in_out_channels):\n",
    "            end = i + 1 == len(self.in_out_channels)\n",
    "            self.downs.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        ResidualBlock(in_channel, in_channel, time_emb_size, groups),\n",
    "                        ResidualBlock(in_channel, in_channel, time_emb_size, groups),\n",
    "                        Residual(\n",
    "                            PreNorm(in_channel, LinearAttention(in_channel))\n",
    "                        ),  # huggingface blog with rms insead of group\n",
    "                        Downsample(in_channel, out_channel, end),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            current_channel = out_channel\n",
    "\n",
    "        for i in range(mid_blocks):\n",
    "            self.middles.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        ResidualBlock(\n",
    "                            current_channel, current_channel, time_emb_size, groups\n",
    "                        ),\n",
    "                        Residual(PreNorm(current_channel, Attention(current_channel))),\n",
    "                        ResidualBlock(\n",
    "                            current_channel, current_channel, time_emb_size, groups\n",
    "                        ),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        for i, (in_channel, out_channel) in enumerate(reversed(self.in_out_channels)):\n",
    "            end = i + 1 == len(self.in_out_channels)\n",
    "            self.ups.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        ResidualBlock(\n",
    "                            out_channel + in_channel, out_channel, time_emb_size, groups\n",
    "                        ),\n",
    "                        ResidualBlock(\n",
    "                            out_channel + in_channel, out_channel, time_emb_size, groups\n",
    "                        ),\n",
    "                        Residual(PreNorm(out_channel, LinearAttention(out_channel))),\n",
    "                        Upsample(out_channel, in_channel, end),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            current_channel = in_channel\n",
    "\n",
    "            self.out1 = ResidualBlock(\n",
    "                current_channel, current_channel, time_emb_size, groups=8\n",
    "            )\n",
    "            self.final = nn.Sequential(\n",
    "                nn.Conv2d(current_channel, current_channel, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(current_channel),\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(current_channel, output_channels, kernel_size=1),\n",
    "            )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = self.initial_conv(x)\n",
    "        t = self.time_embed(t)\n",
    "        skips = []\n",
    "        for res1, res2, att, ds in self.downs:\n",
    "            x = res1(x, t)\n",
    "            skips.append(x)\n",
    "\n",
    "            x = res2(x, t)\n",
    "            x = att(x)\n",
    "            skips.append(x)  # phil wang sends 2 skip connections\n",
    "\n",
    "            x = ds(x)\n",
    "\n",
    "        for res1, att, res2 in self.middles:\n",
    "            x = res1(x, t)\n",
    "            x = att(x)\n",
    "            x = res2(x, t)\n",
    "\n",
    "        scale = 2 ** -0.5\n",
    "        for res1, res2, att, ds in self.ups:\n",
    "            x = torch.cat([x, skips.pop() * scale], dim=1)\n",
    "            x = res1(x, t)\n",
    "\n",
    "            x = torch.cat([x, skips.pop() * scale], dim=1)\n",
    "            x = res2(x, t)\n",
    "\n",
    "            x = att(x)\n",
    "            x = ds(x)\n",
    "\n",
    "        x = self.out1(x, t)\n",
    "        return self.final(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab29ca0",
   "metadata": {},
   "source": [
    "We quickly define the following function that extracts values from a tensor using the given timestep and reshapes it to a specified size which will make later code much easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0975cdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    assert x_shape[0] == b\n",
    "    out = torch.gather(a, dim=0, index=t)\n",
    "    assert out.shape == torch.Size([b])\n",
    "    return out.view(b, *(len(x_shape) - 1) * [1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065fc4a",
   "metadata": {},
   "source": [
    "## The Diffusion Class\n",
    "\n",
    "We define the diffusion class which implements algorithms algorithm 1 for training and algorithm 2 for sampling from the original DDPM paper. Be default we use a cosine schedule due to the relatively smaller image size and channels present in the Strong Gravitation Lensing images and their relative simpliciy(a linear schedule would noise the images way too quickly).\n",
    "\n",
    "I used the default timesteps of 1000 as it seemed to work rather well and this can always be made faster by implementing DDIM sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11784027",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        num_channels=1,\n",
    "        schedule=\"cosine\",\n",
    "        image_size=160,\n",
    "        T=1000,\n",
    "        device=torch.device(\"cuda\"),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.T = T\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        if schedule == \"linear\":\n",
    "            betas = self.linear_betas()\n",
    "        elif schedule == \"cosine\":\n",
    "            betas = self.cosine_betas()\n",
    "        else:\n",
    "            print(f\"Unsupported diffusion schedule: {schedule}\")\n",
    "            sys.exit()\n",
    "        betas = betas.to(torch.float32)\n",
    "        self.register_buffer(\"betas\", betas)\n",
    "        self.register_buffer(\"alphas\", 1 - self.betas)\n",
    "        self.register_buffer(\"alphas_cp\", torch.cumprod(self.alphas, dim=0))\n",
    "        self.register_buffer(\n",
    "            \"alphas_cp_prev\", torch.cat([torch.tensor([1]), self.alphas_cp[:-1]])\n",
    "        )\n",
    "        self.register_buffer(\"sqrt_alphas_cp\", torch.sqrt(self.alphas_cp))\n",
    "        self.register_buffer(\"sqrt_m1_alphas_cp\", torch.sqrt(1 - self.alphas_cp))\n",
    "        self.register_buffer(\"rec_sqrt_alphas\", 1 / torch.sqrt(self.alphas))\n",
    "        self.register_buffer(\"rec_sqrt_m1_alphas_cp\", 1 / self.sqrt_m1_alphas_cp)\n",
    "        self.register_buffer(\"coeffs\", self.betas / self.sqrt_m1_alphas_cp)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"posterior_var\",\n",
    "            self.betas * (1 - self.alphas_cp_prev) / (1 - self.alphas_cp),\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"log_posterior_var\", torch.log(self.posterior_var.clamp(min=1e-15))\n",
    "        )  # first value will be 0 to raise it\n",
    "        self.register_buffer(\n",
    "            \"sqrt_recip_alphas_cumprod\", torch.sqrt(1.0 / self.alphas_cp)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"sqrt_recipm1_alphas_cumprod\", torch.sqrt(1.0 / self.alphas_cp - 1)\n",
    "        )\n",
    "        self.device = device\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def linear_betas(self, start=1e-4, end=0.02):\n",
    "        betas = torch.linspace(start, end, self.T)\n",
    "        return betas\n",
    "\n",
    "    # improved ddpm\n",
    "    def cosine_betas(self, s=0.008):\n",
    "        t = torch.arange(0, self.T + 1, dtype=torch.float64) / self.T\n",
    "        alpha_hats = torch.cos((t + s) / (1 + s) * (pi / 2)).square()\n",
    "        alpha_hats = alpha_hats / alpha_hats[0]\n",
    "        betas = 1 - (alpha_hats[1:] / alpha_hats[:-1])\n",
    "        return torch.clamp(betas, max=0.999)  # prevent singularities\n",
    "\n",
    "    def get_loss(self, x):\n",
    "        b, c, w, h = x.shape\n",
    "\n",
    "        noise = torch.randn_like(x)\n",
    "        times = torch.randint(low=0, high=self.T, size=(b,), device=self.device)\n",
    "        image_mean = x * extract(self.sqrt_alphas_cp, times, x.shape)\n",
    "        image_variance = noise * extract(self.sqrt_m1_alphas_cp, times, x.shape)\n",
    "        image = image_mean + image_variance\n",
    "        output = self.model(image, times)\n",
    "        loss = F.mse_loss(output, noise)\n",
    "        return loss\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            xt = torch.randn((num_samples, self.num_channels, self.image_size, self.image_size)).to(\n",
    "                self.device\n",
    "            )\n",
    "            b, c, w, h = xt.shape\n",
    "            for t in tqdm(reversed(range(self.T)), total=self.T):\n",
    "\n",
    "                t_b = torch.full((b,), t).to(self.device)\n",
    "                z = torch.randn_like(xt)\n",
    "                output = self.model(xt, t_b)\n",
    "                output = xt - output * extract(self.coeffs, t_b, output.shape)\n",
    "                mean = output * extract(self.rec_sqrt_alphas, t_b, output.shape)\n",
    "                if t > 0:\n",
    "                    log_var = extract(self.log_posterior_var, t_b, output.shape)\n",
    "                    variance = z * torch.exp(\n",
    "                        0.5 * log_var\n",
    "                    )  # only have to multiply by 0.5 instead of sqrt\n",
    "                else:\n",
    "                    variance = 0\n",
    "                xt = (mean + variance).to(torch.float32)\n",
    "        self.model.train()\n",
    "        return xt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd8e691",
   "metadata": {},
   "source": [
    "## Training\n",
    "For training, I use mixed precision training due to the high number of convolutions in the network alongside GradScaler which helps deal with any problems that might arise from fp16 training. We also incorporate and EMA that starts after 1000 steps and has a scale of 0.9999 and save the model every 30 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "029b7021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(diffuser, ema, optimizer, scaler, epochs, dataloader, device):\n",
    "    fd = open(\"output-log.txt\", \"w\")\n",
    "    steps = 0\n",
    "    for epoch in range(epochs):\n",
    "        running = 0\n",
    "        total = 0\n",
    "        for image in tqdm(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                loss = diffuser.get_loss(image.to(device))\n",
    "            running += loss.item()\n",
    "            total += 1\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            ema.update()\n",
    "            steps += 1\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(\"Steps: \", steps)\n",
    "        print(f\"Epoch loss: {running / total}\")\n",
    "        fd.write(f\"Epoch: {epoch:>3}, Steps: {steps:>5}, Epoch loss: {running / total}\\n\")\n",
    "        if epoch % 100 == 0:\n",
    "            torch.save(model.module.state_dict(), f\"ddpm-deeplense{epoch}.pt\")\n",
    "            torch.save(ema.online_model.state_dict(), f\"ema-ddpm-deeplense{epoch}.pt\")\n",
    "    fd.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3f515c",
   "metadata": {},
   "source": [
    "Defining the training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a706736",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 109324961\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\") # Training on 4 A40's\n",
    "model = Unet(160, input_channels=1, output_channels=1, channel_mults=[1, 1, 2, 2, 3, 4]) # \n",
    "print(f\"Model size: {sum(p.numel() for p in model.parameters())}\")\n",
    "ema = EMA(\n",
    "    model,\n",
    "    beta = 0.9999,\n",
    "    update_after_step = 1500,\n",
    "    update_every = 10,\n",
    ") # EMA only starts after 1000 steps and updates every 10 steps so that it doesn't slow down training too much\n",
    "ema = ema.to(device)\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "diffuser = Diffusion(model, num_channels=1, schedule=\"cosine\", device=device).to(device) # cosine schedule\n",
    "trainloader = get_dataloader(\"dataset-ddpm/train\", image_size=160, batch_size=155, train=True, num_workers=1) # num_workers doesn't have to be high\n",
    "testloader = get_dataloader(\"dataset-ddpm/val\", image_size=160, batch_size=155, train=False, num_workers=1) # num_workers doesn't have to be high\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4) # same as used in guided diffusion paper\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b41a5b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(diffuser, ema, optimizer, scaler, epochs, dataloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
